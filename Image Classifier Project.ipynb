{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing an AI application for the identification of flower species\n",
    "\n",
    "Ih this project, I will train an image classifier to recognize different species of flowers, then generate and save the corresponding model, and further use it to predict flower species from an independent flower image. The model will use [this dataset](http://www.robots.ox.ac.uk/~vgg/data/flowers/102/index.html) of 102 flower categories for training. A few examples are presented below. \n",
    "\n",
    "<img src='assets/Flowers.png' width=500px>\n",
    "\n",
    "The project is broken down into multiple steps:\n",
    "\n",
    "* Load and preprocess the image dataset\n",
    "* Load a pre-trained model\n",
    "* Train the image classifier on the dataset\n",
    "* Use the trained classifier to predict image content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary modules\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from time import gmtime, strftime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "\n",
    "The dataset is split into two parts, training and validation. For the training, I will apply transformations such as random scaling, cropping, and flipping, to help the network generalize leading to better performance. To make sure the images are fit for the pre-trained network, the input data is resized to 224x224 pixels. The validation set is only resized then cropped to the appropriate size.\n",
    "\n",
    "The pre-trained networks available from `torchvision` were trained on the ImageNet dataset where each color channel was normalized separately. For both datasets, I will therefore normalize the means and standard deviations of the images to what the network expects. For the means, it's `[0.485, 0.456, 0.406]` and for the standard deviations `[0.229, 0.224, 0.225]`, calculated from the ImageNet images.  These values will shift each color channel to be centered at 0 and range from -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining directories\n",
    "\n",
    "data_dir = 'flower_data'\n",
    "train_dir = data_dir + '/train'\n",
    "valid_dir = data_dir + '/valid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining parameters\n",
    "\n",
    "random_rotation = 20\n",
    "resize = 224\n",
    "center_crop = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the transforms for the training and validation sets\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(random_rotation),\n",
    "    transforms.RandomResizedCrop(resize),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "valid_transforms = transforms.Compose([\n",
    "    transforms.Resize(resize),\n",
    "    transforms.CenterCrop(center_crop),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the datasets with ImageFolder\n",
    "\n",
    "train_dataset = torchvision.datasets.ImageFolder(train_dir, transform = train_transforms)\n",
    "valid_dataset = torchvision.datasets.ImageFolder(valid_dir, transform = valid_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the batch size so that I will have 60 batches\n",
    "\n",
    "batch_size = int(len(train_dataset)/60)\n",
    "\n",
    "# Defining the data loaders\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label mapping\n",
    "\n",
    "I will load in a mapping from category label to category name from the file `cat_to_name.json`. This will give me a dictionary mapping the integer encoded categories to the actual names of the flowers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('assets/cat_to_name.json', 'r') as f:\n",
    "    cat_to_name = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and training the classifier\n",
    "\n",
    "In order to build and train the classifier, I will use the pre-trained ResNet512 model from `torchvision.models` to get the image features. The whole process will be accomplished according to the following steps:\n",
    "\n",
    "* Load the [pre-trained network](http://pytorch.org/docs/master/torchvision/models.html)\n",
    "* Define a new, untrained feed-forward network as a classifier, using ReLU activations and dropout\n",
    "* Train the classifier layers using backpropagation using the pre-trained network to get the features\n",
    "* Track the loss and accuracy on the validation set\n",
    "* The model with the lowest loss will be saved as a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building and training the classifier\n",
    "\n",
    "model = models.resnet152(pretrained = True)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Defining the classifier class\n",
    "\n",
    "class FCClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, middle_features_1, middle_features_2, out_features, drop_prob = 0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features, middle_features_1, bias = True)\n",
    "        self.fc2 = nn.Linear(middle_features_1, middle_features_2, bias = True)\n",
    "        self.fc3 = nn.Linear(middle_features_2, out_features, bias = True)\n",
    "        self.drop = nn.Dropout(p = drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.drop(F.relu(self.fc1(x)))\n",
    "        x = self.drop(F.relu(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "# Defining the classifier in my model, using the classifier class created above\n",
    "\n",
    "model.fc = FCClassifier(2048, 2048, 1024, 102)\n",
    "\n",
    "# Defining the loss and optimizer functions\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "lr = 0.001\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr = lr)\n",
    "\n",
    "# Moving the model to GPU\n",
    "\n",
    "model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to save the model, the classifier, and the \"model class to folder index\" dictionary\n",
    "\n",
    "def save_checkpoint(model, filename):\n",
    "    checkpoint = {'model': 'resnet152',\n",
    "                  'classifier': model.fc,\n",
    "                  'model_state_dict': model.state_dict(),\n",
    "                  'class_to_idx': train_dataset.class_to_idx}\n",
    "    torch.save(checkpoint, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining parameters for training\n",
    "\n",
    "epochs = 10\n",
    "steps = 0\n",
    "total_steps = len(train_loader) * epochs\n",
    "running_loss = 0\n",
    "print_every = 5\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "# Training\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Loading a batch of training images\n",
    "    for images, labels in train_loader:\n",
    "        steps += 1\n",
    "        \n",
    "        # Moving images and labels to GPU\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "        \n",
    "        # Training the model and adjusting the weights\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logps = model(images)\n",
    "        loss = criterion(logps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Keeping track of the training loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Check the validation set at the first, the last and at each 5 training batches\n",
    "        if (steps == 1) or (steps % print_every == 0) or (steps == total_steps):\n",
    "            model.eval()\n",
    "            \n",
    "            test_loss = 0\n",
    "            accuracy = 0\n",
    "            \n",
    "            # Loading a batch of validation images\n",
    "            for images, labels in valid_loader:\n",
    "                images, labels = images.cuda(), labels.cuda()\n",
    "                \n",
    "                # Predicting and calculating validation loss\n",
    "                logps = model(images)\n",
    "                loss = criterion(logps, labels)\n",
    "                test_loss += loss.item()\n",
    "                \n",
    "                # Calculating accuracy\n",
    "                ps = torch.exp(logps)\n",
    "                top_ps, top_class = ps.topk(1, dim = 1)\n",
    "                equality = top_class == labels.view(*top_class.shape)\n",
    "                accuracy += torch.mean(equality.type(torch.FloatTensor)).item()\n",
    "            \n",
    "            # Printing partial measures\n",
    "            print(f'{strftime(\"%H:%M:%S\", gmtime())}.. '\n",
    "                  f'Epoch {epoch+1}/{epochs}.. '\n",
    "                  f'Step {steps}/{total_steps}.. '\n",
    "                  f'Train loss: {running_loss/print_every:.3f}.. '\n",
    "                  f'Test loss: {test_loss/len(valid_loader):.3f}.. '\n",
    "                  f'Test accuracy: {accuracy/len(valid_loader):.3f}')\n",
    "            \n",
    "            running_loss = 0\n",
    "            model.train()\n",
    "            \n",
    "            # Saving the model if the lowest validation loss so far has been reached\n",
    "            if test_loss/len(valid_loader) <= valid_loss_min:\n",
    "                print('Saving model ... ', end=\"\")\n",
    "                save_checkpoint(model, 'checkpoint.pth')\n",
    "                print('Done!')\n",
    "                valid_loss_min = test_loss/len(valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the checkpoint\n",
    "\n",
    "I will load the saved checkpoint, which consists of the model with the lowest validation loss, so I can use it to predict the species of new flower images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to load the checkpoint and format the model\n",
    "\n",
    "def load_checkpoint(filename):\n",
    "    checkpoint = torch.load(filename, map_location='cpu')\n",
    "    model = models.resnet152(pretrained = True)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    model.fc = checkpoint['classifier']\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.class_to_idx = checkpoint['class_to_idx']\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the checkpoint and transfering the model to GPU\n",
    "\n",
    "model = load_checkpoint('checkpoint.pth')\n",
    "model.cuda();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference for classification\n",
    "\n",
    "I am providing 7 test images in the folder `assets`, labeled testX.jpg, where X = 1 - 7. Nevertheless, any flower image from any source can be tested.\n",
    "\n",
    "In order to infer flower species, first I will write a function to preprocess any input image, so that it can be fed to the model.\n",
    "\n",
    "First, the images will be resized where the shortest side is 256 pixels, keeping the aspect ratio. Then they will be center-cropped to generate 224x224 images.\n",
    "\n",
    "Color channels of images are typically encoded as integers 0-255, but the model expected floats 0-1, therefore they will be converted likewise.\n",
    "\n",
    "As before, the network expects the images to be normalized in a specific way. For the means, it's `[0.485, 0.456, 0.406]` and for the standard deviations `[0.229, 0.224, 0.225]`. I will subtract the means from each color channel, then divide by the standard deviation. \n",
    "\n",
    "And finally, PyTorch expects the color channel to be the first dimension but it's the third dimension in the PIL image and Numpy array. The color channel needs to be first and retain the order of the other two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to preprocess the image\n",
    "\n",
    "def process_image(image):\n",
    "    ''' Scales, crops, and normalizes a PIL image for a PyTorch model,\n",
    "        returns an Numpy array\n",
    "    '''\n",
    "    \n",
    "    img = image\n",
    "    \n",
    "    # Resize\n",
    "    ratio = min(img.size)/256\n",
    "    img = img.resize((int(img.size[0]/ratio), int(img.size[1]/ratio)))\n",
    "    \n",
    "    # Center crop\n",
    "    center = (int(img.size[0]/2), int(img.size[1]/2))\n",
    "    left, right, top, bottom = int(center[0]-224/2), int(center[0]+224/2), int(center[1]-224/2), int(center[1]+224/2)\n",
    "    img = img.crop((left, top, right, bottom))\n",
    "    \n",
    "    # Convert to numpy\n",
    "    img = np.array(img, dtype = 'float')\n",
    "    img = img/255.\n",
    "        \n",
    "    # Normalize\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    sd = np.array([0.229, 0.224, 0.225])\n",
    "    img = (img - mean)/sd\n",
    "        \n",
    "    # Transpose\n",
    "    img = img.transpose((2, 0, 1))\n",
    "    \n",
    "    # Change to tensor\n",
    "    img = torch.tensor(img).cuda()\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Prediction\n",
    "\n",
    "I will write a function that accepts an image and a model, and returns thre lists. The first list will give the highest 5 probabilities for class predicted by the model. The second list will give the corresponding 5 top classes, as defined by the ImageFolder loader. The third list will give the corresponding 5 top indices, mapping to the folder number. This last list can then be translated into flower species using the `cat_to_name` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the prediction function\n",
    "\n",
    "def predict(image_path, model, topk=5):\n",
    "    ''' Predict the class (or classes) of an image using a trained deep learning model.\n",
    "    '''\n",
    "    from PIL import Image\n",
    "    img = Image.open(image_path)\n",
    "\n",
    "    img = process_image(img)\n",
    "    img = img.unsqueeze(0)\n",
    "    img = img.float()\n",
    "    \n",
    "    # TODO: Implement the code to predict the class from an image file\n",
    "    \n",
    "    model2.eval()\n",
    "    \n",
    "    logps = model2(img)\n",
    "    ps = torch.exp(logps)\n",
    "    \n",
    "    top_ps, top_class = ps.topk(topk, dim = 1)\n",
    "    probs = top_ps.cpu().detach().numpy().tolist()[0]\n",
    "\n",
    "    classes = top_class.cpu().detach().numpy().tolist()[0]\n",
    "\n",
    "    class_to_idx = model2.class_to_idx\n",
    "    idx_to_class = {i: ii for ii, i in class_to_idx.items()}\n",
    "    \n",
    "    indexes = []\n",
    "    for i in range(len(classes)):\n",
    "        index = idx_to_class[classes[i]]\n",
    "        indexes.append(index)\n",
    "        \n",
    "    return probs, classes, indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualing the results\n",
    "\n",
    "The function below converts a PyTorch tensor and displays it in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a funciton to view the test image\n",
    "\n",
    "def imshow(image, ax=None, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    \n",
    "    # PyTorch tensors assume the color channel is the first dimension\n",
    "    # but matplotlib assumes is the third dimension\n",
    "    image = image.numpy().transpose((1, 2, 0))\n",
    "    \n",
    "    # Undo preprocessing\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    image = std * image + mean\n",
    "    \n",
    "    # Image needs to be clipped between 0 and 1 or it looks like noise when displayed\n",
    "    image = np.clip(image, 0, 1)\n",
    "    \n",
    "    ax.imshow(image)\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use the model on the `test1.jpg` image. The results will be shown in a barplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the results\n",
    "\n",
    "test = 'assets/test1.jpg'\n",
    "probs, classes, indexes = predict(test, model2)\n",
    "names = []\n",
    "for i in range(len(classes)):\n",
    "    name = cat_to_name[indexes[i]]\n",
    "    names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the image plot\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "img = process_image(Image.open(test)).cpu()\n",
    "imshow(img, ax1, title = names[0])\n",
    "ax2.barh(names, probs)\n",
    "ax2.invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
